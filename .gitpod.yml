tasks:
  - name: Setup Ollama
    init: |
      # Create a local bin folder for Ollama
      mkdir -p $HOME/bin
      curl -fL https://ollama.com/download/ollama-linux-amd64 -o $HOME/bin/ollama
      chmod +x $HOME/bin/ollama

      # Persist models in the workspace
      mkdir -p /workspace/.ollama

      # Add Ollama to PATH for all future shells
      echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
      echo 'export OLLAMA_MODELS=/workspace/.ollama' >> ~/.bashrc

    command: |
      # Load environment variables
      export PATH=$HOME/bin:$PATH
      export OLLAMA_MODELS=/workspace/.ollama

      # Start Ollama service in background
      nohup ollama serve > /tmp/ollama.log 2>&1 &

      # Wait until Ollama API is ready
      echo "Waiting for Ollama to start..."
      for i in {1..10}; do
        if curl -s http://localhost:11434/api/tags > /dev/null; then
          echo "Ollama is ready!"
          break
        fi
        sleep 2
      done

      # Pull llama3 model
      ollama pull llama3 || echo "Could not pull llama3 model."

ports:
  - port: 11434
    onOpen: ignore
    visibility: public
